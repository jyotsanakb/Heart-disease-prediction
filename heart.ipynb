                                                                        
# Importing essential libraries
import numpy as np
import pandas as pd



# Loading the dataset
df = pd.read_csv("https://raw.githubusercontent.com/anujvyas/Machine-Learning-Projects/master/Heart%20Disease%20Prediction/heart.csv")
df.head()



# Returns number of rows and columns of the dataset
df.shape


# Returns an object with all of the column headers
df.columns


# Returns different datatypes for each columns (float, int, string, bool, etc.)
df.dtypes


# Returns the first x number of rows when head(x). Without a number it returns 5
df.head()


# Returns the last x number of rows when tail(x). Without a number it returns 5
df.tail()


# Returns true for a column having null values, else false
df.isnull().any()


# Returns basic information on all columns
df.info()
# Returns basic statistics on numeric columns
df.describe().T
# Importing essential libraries
import matplotlib.pyplot as plt
%matplotlib inline
import seaborn as sns
# Plotting histogram for the entire dataset
fig = plt.figure(figsize = (15,15))
ax = fig.gca()
g = df.hist(ax=ax)


# Visualization to check if the dataset is balanced or not
g = sns.countplot(x='target', data=df)
plt.xlabel('Target')
plt.ylabel('Count')


# Selecting correlated features using Heatmap

# Get correlation of all the features of the dataset
corr_matrix = df.corr()
top_corr_features = corr_matrix.index

# Plotting the heatmap
plt.figure(figsize=(20,20))
sns.heatmap(data=df[top_corr_features].corr(), annot=True, cmap='RdYlGn')

#FINDING DUMMY VARIABLES
dataset = pd.get_dummies(df, columns=['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'ca', 'thal'])

#Printig columns in the dataset.
dataset.columns

#Importing standardScaler.
from sklearn.preprocessing import StandardScaler
standScaler = StandardScaler()
columns_to_scale = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak']
dataset[columns_to_scale] = standScaler.fit_transform(dataset[columns_to_scale])
dataset.head()


# Splitting the dataset into dependent and independent features
X = dataset.drop('target', axis=1)
y = dataset['target']

#KneighborsClassifier.

# Importing essential libraries
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import cross_val_score
# Finding the best accuracy for knn algorithm using cross_val_score 
knn_scores = []
for i in range(1, 21):
  knn_classifier = KNeighborsClassifier(n_neighbors=i)
  cvs_scores = cross_val_score(knn_classifier, X, y, cv=10)
  knn_scores.append(round(cvs_scores.mean(),3))
# Plotting the results of knn_scores
plt.figure(figsize=(16,8))
plt.plot([k for k in range(1, 21)], knn_scores, color = 'red')
for i in range(1,21):
    plt.text(i, knn_scores[i-1], (i, knn_scores[i-1]))
plt.xticks([i for i in range(1, 21)])
plt.xlabel('Number of Neighbors (K)')
plt.ylabel('Scores')
plt.title('K Neighbors Classifier scores for different K values')


#Decision tree classifier
# Importing essential libraries
from sklearn.tree import DecisionTreeClassifier

# Finding the best accuracy for decision tree algorithm using cross_val_score 
decision_scores = []
for i in range(1, 11):
  decision_classifier = DecisionTreeClassifier(max_depth=i)
  cvs_scores = cross_val_score(decision_classifier, X, y, cv=10)
  decision_scores.append(round(cvs_scores.mean(),3))


# Plotting the results of decision_scores
plt.figure(figsize=(16,8))
plt.plot([i for i in range(1, 11)], decision_scores, color = 'red')
for i in range(1,11):
    plt.text(i, decision_scores[i-1], (i, decision_scores[i-1]))
plt.xticks([i for i in range(1, 11)])
plt.xlabel('Depth of Decision Tree (N)')
plt.ylabel('Scores')
plt.title('Decision Tree Classifier scores for different depth values')

# Training the decision tree classifier model with max_depth value as 3
decision_classifier = DecisionTreeClassifier(max_depth=3)
cvs_scores = cross_val_score(decision_classifier, X, y, cv=10)
print("Decision Tree Classifier Accuracy with max_depth=3 is: {}%".format(round(cvs_scores.mean(), 4)*100))

#AdaBoost classifier
from sklearn.ensemble import AdaBoostClassifier

adab_scores = []
for i in range(1, 11):
  adab_classifier = AdaBoostClassifier(n_estimators=100, random_state=0)
  cvs_scores = cross_val_score(adab_classifier, X, y, cv=10)
  adab_scores.append(round(cvs_scores.mean(),3))

# Plotting the results of adab_scores
plt.figure(figsize=(16,8))
plt.plot([i for i in range(1, 11)], adab_scores, color = 'red')
for i in range(1,11):
    plt.text(i, adab_scores[i-1], (i, adab_scores[i-1]))
plt.xticks([i for i in range(1, 11)])
plt.xlabel('Depth of adab (N)')
plt.ylabel('Scores')
plt.title('adab  scores for different depth values')

#Accuracy percentage.
adab_classifier = AdaBoostClassifier()
cvs_scores = cross_val_score(adab_classifier, X, y, cv=10)
print("adab Tree Classifier Accuracy with max_depth=3 is: {}%".format(round(cvs_scores.mean(), 4)*100))


#Navie bayes classifier

import csv
import pandas as pd
import numpy as np
from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import train_test_split
from sklearn import metrics
from sklearn.metrics import confusion_matrix, f1_score, roc_curve, auc
import matplotlib.pyplot as plt
from itertools import cycle
from scipy import interp

# reading CSV using Pandas and storing in dataframe       
df = pd.read_csv('https://raw.githubusercontent.com/MastersAbh/Heart-Disease-Prediction-using-Naive-Bayes-Classifier/master/heartdisease.csv', header = None)

training_x=df.iloc[1:df.shape[0],0:13]
#print(training_set)

training_y=df.iloc[1:df.shape[0],13:14]
#print(testing_set)

# converting dataframe into arrays
x=np.array(training_x)
y=np.array(training_y)

for z in range(5):
    print("\n\n\nTest Train Split no. ",z+1,"\n\n\n")
    x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.25,random_state=None)
    # Gaussian function of sklearn
    gnb = GaussianNB()
    gnb.fit(x_train, y_train.ravel())
    y_pred = gnb.predict(x_test)
    
    print("\n\nGaussian Naive Bayes model accuracy(in %):", metrics.accuracy_score(y_test, y_pred)*100)
    
    # convert 2D array to 1D array
    y1=y_test.ravel()
    y_pred1=y_pred.ravel()
    
    print("\n\n\n\nConfusion Matrix")
    cf_matrix=confusion_matrix(y1,y_pred1)
    print(cf_matrix)
    
    print("\n\n\n\nF1 Score")
    f_score=f1_score(y1,y_pred1,average='weighted')
    print(f_score)
    
    # Matrix from 1D array
    y2=np.zeros(shape=(len(y1),5))
    y3=np.zeros(shape=(len(y_pred1),5))
    for i in range(len(y1)):
        y2[i][int(y1[i])]=1
    
    for i in range(len(y_pred1)):
        y3[i][int(y_pred1[i])]=1
     
    
    # ROC Curve generation
    n_classes = 5
    
    fpr = dict()
    tpr = dict()
    roc_auc = dict()
    for i in range(n_classes):
        fpr[i], tpr[i], _ = roc_curve(y2[:, i], y3[:, i])
        roc_auc[i] = auc(fpr[i], tpr[i])
        
    # Compute micro-average ROC curve and ROC area
    fpr["micro"], tpr["micro"], _ = roc_curve(y2.ravel(), y3.ravel())
    roc_auc["micro"] = auc(fpr["micro"], tpr["micro"])
    
    # Compute macro-average ROC curve and ROC area
    print("\n\n\n\nROC Curve")
    # First aggregate all false positive rates
    lw=2
    all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))
    
    # Then interpolate all ROC curves at this points
    mean_tpr = np.zeros_like(all_fpr)
    for i in range(n_classes):
        mean_tpr += interp(all_fpr, fpr[i], tpr[i])
    
    # Finally average it and compute AUC
    mean_tpr /= n_classes
    
    fpr["macro"] = all_fpr
    tpr["macro"] = mean_tpr
    roc_auc["macro"] = auc(fpr["macro"], tpr["macro"])
    
    # Plot all ROC curves
    plt.figure()
    plt.plot(fpr["micro"], tpr["micro"],
             label='micro-average (area = {0:0.2f})'
                   ''.format(roc_auc["micro"]),
             color='deeppink', linestyle=':', linewidth=4)
    
    plt.plot(fpr["macro"], tpr["macro"],
             label='macro-average (area = {0:0.2f})'
                   ''.format(roc_auc["macro"]),
             color='navy', linestyle=':', linewidth=4)
    
    colors = cycle(['aqua', 'darkorange', 'cornflowerblue','red','black'])
    for i, color in zip(range(n_classes), colors):
        plt.plot(fpr[i], tpr[i], color=color, lw=lw,
                 label='ROC of class {0} (area = {1:0.2f})'
                 ''.format(i, roc_auc[i]))
    
    plt.plot([0, 1], [0, 1], 'k--', lw=lw)
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Receiver operating characteristic for multi-class')
    plt.legend(loc="lower right")
    plt.show()
    












